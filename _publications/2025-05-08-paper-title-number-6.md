---
title: "Efficient Class-Incremental Segmentation Learning via Expanding Visual Transformers"
collection: publications
category: conferences
permalink: /publication/TILES
excerpt: '<img src="/images/visu_tiles.png" alt="A diagram illustrating the MVAT framework" style="float: left; margin: 0em 1em 0em 0em; width: 450px;"> Incrementally learning new semantic concepts while retaining existing information is fundamental for several real-world applications. Although the impact of backbone size and architectural choices has been extensively studied in non-incremental computer vision tasks for efficiency concerns, class-incremental semantic segmentation models have so far focused primarily on large backbones, without offering a fair comparison in terms of model size. In this work, we propose a fairer study across existing class-incremental semantic segmentation methods, focusing on the models efficiency with respect to their memory footprint. Moreover, we propose TILES (Transformer-based Incremental Learning for Expanding Segmenter), a novel approach exploiting small-size ViT backbones efficiency to offer an alternative solution where severe memory constraints are applied. It is based on expanding the architecture with the increments, allowing to learn new tasks while retaining old knowledge within a limited memory footprint. Besides, in order to tackle the background semantic shift, we apply adaptive losses specific to the incremental branches, while balancing old and new knowledge. Furthermore, we exploit the confidence of each incremental task to propose an efficient branch merging strategy. TILES outperforms several previous methods on various challenging benchmarks while using up to 14 times fewer parameters.'
date: 2025-02-01
venue: 'Under review'
paperurl: 
citation: 
---
Incrementally learning new semantic concepts while retaining existing information is fundamental for several real-world applications. Although the impact of backbone size and architectural choices has been extensively studied in non-incremental computer vision tasks for efficiency concerns, class-incremental semantic segmentation models have so far focused primarily on large backbones, without offering a fair comparison in terms of model size. In this work, we propose a fairer study across existing class-incremental semantic segmentation methods, focusing on the models efficiency with respect to their memory footprint. Moreover, we propose TILES (Transformer-based Incremental Learning for Expanding Segmenter), a novel approach exploiting small-size ViT backbones efficiency to offer an alternative solution where severe memory constraints are applied. It is based on expanding the architecture with the increments, allowing to learn new tasks while retaining old knowledge within a limited memory footprint. Besides, in order to tackle the background semantic shift, we apply adaptive losses specific to the incremental branches, while balancing old and new knowledge. Furthermore, we exploit the confidence of each incremental task to propose an efficient branch merging strategy. TILES outperforms several previous methods on various challenging benchmarks while using up to 14 times fewer parameters.